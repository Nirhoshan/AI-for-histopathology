{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e8b8dc5-de39-4fa1-84d2-efd8d04e11ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append('./')\n",
    "from utils import distribute_over_GPUs, validate_arguments\n",
    "from model import Model, Identity\n",
    "from get_dataloader import get_dataloader\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve,average_precision_score, precision_recall_curve,auc\n",
    "import h5py\n",
    "from losses import edl_mse_loss, edl_digamma_loss, edl_log_loss, relu_evidence\n",
    "from helpers import get_device, rotate_img, one_hot_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6229af39-d7f1-4cdf-bbed-8998deca38a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Record:\n",
    "    def __init__(self):\n",
    "        self.dataset = \"nct100k\"\n",
    "        self.save_dir = \"new_result\"\n",
    "        self.data_input_dir = \"/n/holyscratch01/wadduwage_lab/Nirho\"\n",
    "        self.data_input_dir_test = \"/n/holyscratch01/wadduwage_lab/Nirho\"\n",
    "        self.log_path = \"new_result\"\n",
    "        self.model_path = \"distilled_model.pth\"\n",
    "        self.num_classes = 9\n",
    "        self.training_data_csv = \"nct100k.csv\"\n",
    "        self.test_data_csv = \"crc7k.csv\"\n",
    "        self.validation_data_csv = \"crc7k.csv\"\n",
    "        self.trainingset_split = None\n",
    "        self.balanced_training_set = False\n",
    "        self.balanced_validation_set = False\n",
    "        self.train_supervised = True\n",
    "        self.batch_size = 128\n",
    "        self.batch_size_multiGPU = 512\n",
    "        self.num_workers = 40\n",
    "        self.pretrained = False\n",
    "        self.finetune = False\n",
    "        self.grayscale = False\n",
    "        self.seed = 44\n",
    "        self.uncertainty = True\n",
    "        self.epochs = 1\n",
    "        self.image_size = 224\n",
    "        self.use_album = False\n",
    "        self.weight_decay = 1e-6\n",
    "        self.lr = 1e-3\n",
    "        self.device = \" \"\n",
    "        self.scale = [0.2, 1.0]\n",
    "        self.rgb_gaussian_blur_p = 0\n",
    "        self.rgb_jitter_d = 1\n",
    "        self.rgb_jitter_p = 0.8\n",
    "        self.rgb_contrast = 0.2\n",
    "        self.rgb_contrast_p = 0\n",
    "        self.rgb_grid_distort_p = 0\n",
    "        self.rgb_grid_shuffle_p = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5750d67b-af36-4d46-b2e3-1f8273e80a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "opt=Record()\n",
    "output_dims = 1024\n",
    "if not os.path.exists(opt.save_dir):\n",
    "    os.makedirs(opt.save_dir, exist_ok=True)\n",
    "with open(f'{opt.log_path}/metadata_train.txt', 'w') as metadata_file:\n",
    "    metadata_file.write(json.dumps(vars(opt)))\n",
    "opt.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', opt.device)\n",
    "random.seed(opt.seed)\n",
    "np.random.seed(opt.seed)\n",
    "torch.manual_seed(opt.seed)\n",
    "torch.cuda.manual_seed(opt.seed)\n",
    "torch.cuda.manual_seed_all(opt.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b5031fb-ef35-4250-bf65-491bdd6ddcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark=True\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Load pre-trained model\n",
    "        base_model = Model(pretrained=opt.pretrained)\n",
    "        self.f = base_model.f\n",
    "        self.g1 = base_model.g1\n",
    "        # classifier\n",
    "        self.fc = nn.Linear(output_dims, opt.num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.f(x)\n",
    "        feature = torch.flatten(x, start_dim=1)\n",
    "        feature = self.g1(feature)\n",
    "        out = self.fc(feature)\n",
    "        return out,feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "29615081-293f-4a65-8e41-2baa42158e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val(net, data_loader, train_optimizer,device,num_classes,uncertainty=False):\n",
    "    is_train = train_optimizer is not None\n",
    "    net.eval() # train only the last layers.\n",
    "    #net.train() if is_train else net.eval()\n",
    "\n",
    "    total_loss, total_correct, total_num, data_bar = 0.0, 0.0, 0, tqdm(data_loader)\n",
    "    all_preds, all_labels, all_slides, all_patches  = [], [], [], []\n",
    "    all_evidence=[]\n",
    "    all_evidence=torch.FloatTensor(all_evidence)\n",
    "    all_evidence = all_evidence.cuda(non_blocking=True)\n",
    "\n",
    "    with (torch.enable_grad() if is_train else torch.no_grad()):\n",
    "        for data, _, target, patch_id, slide_id in data_bar:\n",
    "            data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)\n",
    "            if uncertainty:\n",
    "                out,feature = model(data)\n",
    "                _, preds = torch.max(out.data, 1)\n",
    "                evidence = relu_evidence(out)\n",
    "                all_evidence=torch.cat((all_evidence,evidence),dim=0)\n",
    "                total_evidence = torch.sum(evidence, 1, keepdim=True)\n",
    "            else:\n",
    "                out,feature = net(data)\n",
    "                _, preds = torch.max(out.data, 1)\n",
    "            if is_train:\n",
    "                train_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                train_optimizer.step()\n",
    "            _, preds = torch.max(out.data, 1)\n",
    "\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(target.cpu().data.numpy())\n",
    "            all_patches.extend(patch_id)\n",
    "            all_slides.extend(slide_id)\n",
    "\n",
    "            probs = torch.nn.functional.softmax(out.data, dim=1).cpu().numpy()\n",
    "         \n",
    "            total_num += data.size(0)\n",
    "            prediction = torch.argsort(out, dim=-1, descending=True)\n",
    "            total_correct += torch.sum((prediction[:, 0:1] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n",
    "            \n",
    "            data_bar.set_description(f'{\"Train\" if is_train else \"Test\"} Epoch: [1] ACC: {total_correct / total_num * 100:.2f}')\n",
    "\n",
    "    if uncertainty:     \n",
    "        alpha = all_evidence + 1\n",
    "        u = num_classes / torch.sum(alpha, dim=1, keepdim=True)\n",
    "        prob = alpha / torch.sum(alpha, dim=1, keepdim=True)\n",
    "        u=u.cpu().data.numpy()\n",
    "        u2=u[:, 0]  \n",
    "        df =  pd.DataFrame({\n",
    "                'label': all_labels,\n",
    "                'prediction': all_preds,\n",
    "                'slide_id': all_slides,\n",
    "                'patch_id': all_patches,\n",
    "                'uncertainty' : u2\n",
    "            })\n",
    "    else:\n",
    "        df =  pd.DataFrame({\n",
    "                'label': all_labels,\n",
    "                'prediction': all_preds,\n",
    "                'slide_id': all_slides,\n",
    "                'patch_id': all_patches\n",
    "        })\n",
    "    return total_correct / total_num * 100, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58215ddf-f74b-4039-a583-11498bff47c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home02/snirhoshan/.conda/envs/evident2/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/n/home02/snirhoshan/.conda/envs/evident2/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 4 GPUs!\n",
      "opt.data_input_dir:  /n/holyscratch01/wadduwage_lab/Nirho\n",
      "opt.data_input_dir_test:  /n/holyscratch01/wadduwage_lab/Nirho\n",
      "reading csv file:  nct100k.csv\n",
      "reading csv file:  crc7k.csv\n",
      "reading csv file:  crc7k.csv\n",
      "Removing non-existing file from dataset: /n/holyscratch01/wadduwage_lab/Nirho/NCT-CRC-HE-100K/ADI/.DS_Store\n",
      "training patches:  label\n",
      "ADI     10407\n",
      "BACK    10566\n",
      "DEB     11512\n",
      "LYM     11557\n",
      "MUC      8896\n",
      "MUS     13536\n",
      "NORM     8763\n",
      "STR     10446\n",
      "TUM     14317\n",
      "dtype: int64\n",
      "Validation patches:  label\n",
      "ADI     1338\n",
      "BACK     847\n",
      "DEB      339\n",
      "LYM      634\n",
      "MUC     1035\n",
      "MUS      592\n",
      "NORM     741\n",
      "STR      421\n",
      "TUM     1233\n",
      "dtype: int64\n",
      "Test patches:  label\n",
      "ADI     1338\n",
      "BACK     847\n",
      "DEB      339\n",
      "LYM      634\n",
      "MUC     1035\n",
      "MUS      592\n",
      "NORM     741\n",
      "STR      421\n",
      "TUM     1233\n",
      "dtype: int64\n",
      "Saving training/val set to file\n",
      "{'ADI': 0, 'MUC': 1, 'BACK': 2, 'LYM': 3, 'NORM': 4, 'DEB': 5, 'MUS': 6, 'TUM': 7, 'STR': 8}\n",
      "{'ADI': 0, 'MUC': 1, 'BACK': 2, 'LYM': 3, 'NORM': 4, 'DEB': 5, 'MUS': 6, 'TUM': 7, 'STR': 8}\n",
      "{'ADI': 0, 'MUC': 1, 'BACK': 2, 'LYM': 3, 'NORM': 4, 'DEB': 5, 'MUS': 6, 'TUM': 7, 'STR': 8}\n",
      "Weighted validation sampler\n",
      "{'ADI': 0.0007473841554559044, 'BACK': 0.0011806375442739079, 'DEB': 0.0029498525073746312, 'LYM': 0.0015772870662460567, 'MUC': 0.000966183574879227, 'MUS': 0.0016891891891891893, 'NORM': 0.001349527665317139, 'STR': 0.0023752969121140144, 'TUM': 0.0008110300081103001}\n",
      "Weighted training sampler\n",
      "{'ADI': 9.608917075045643e-05, 'BACK': 9.46431951542684e-05, 'DEB': 8.686587908269631e-05, 'LYM': 8.652764558276369e-05, 'MUC': 0.00011241007194244605, 'MUS': 7.387706855791962e-05, 'NORM': 0.00011411617026132602, 'STR': 9.573042312847023e-05, 'TUM': 6.984703499336453e-05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch: [1] ACC: 96.44: 100%|██████████| 28/28 [00:51<00:00,  1.85s/it]\n"
     ]
    }
   ],
   "source": [
    "model = Net(opt)\n",
    "model, num_GPU = distribute_over_GPUs(opt, model)\n",
    "train_loader, train_data, val_loader, val_data, test_loader, test_data = get_dataloader(opt)\n",
    "\n",
    "if not opt.finetune:\n",
    "    for param in model.module.f.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=opt.lr, weight_decay=opt.weight_decay)\n",
    "\n",
    "scheduler = CosineAnnealingLR(optimizer, opt.epochs)\n",
    "\n",
    "criterion = edl_mse_loss\n",
    "loss_criterion = nn.CrossEntropyLoss().to(opt.device)\n",
    "results = {'train_acc': [],'val_acc': []}\n",
    "model.load_state_dict(torch.load(f'{opt.model_path}'))\n",
    "model.eval()\n",
    "test_acc, df = train_val(model, test_loader, None,opt.device,opt.num_classes,uncertainty=opt.uncertainty)\n",
    "df.to_csv(f\"{opt.log_path}/inference_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4014be4c-226f-44d6-a0e1-d680b5870d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e7873e-7d21-43ff-81f5-6796442973e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evident2",
   "language": "python",
   "name": "evident2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
